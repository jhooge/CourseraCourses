---
title: "Linear Regression on Car Transmission Type and Fuel Consumption"
author: "Jens Hooge"
date: "21.9.2014"
output: html_document
---

Executive Summary
---

The goal of this analysis is tho answer the question, whether automatic or 
manual transmission cars have a significant influence on fuel consumption. In 
particular we are interested in exploring the relationship between a set of 
variables and miles per gallon (MPG) (outcome). There fore we will **quantify 
the MPG difference between automatic and manual transmissions** and try to 
answer the question, **whether an automatic or manual transmission is better for 
MPG.**

Using a number of linear modeling techniques together with hypothesis testing,
we concluded, that on average cars with manual transmission run **7.3 miles per
gallon** farther than cars with automatic transmission. We could show, that this
difference is mostly influenced by the car's **displacement, horsepower and 
weight**.

Note: Due to spacial constraints, raw R code won't be displayed here. Please 
refer to [GitHub]("https://github.com/jhooge/CourseraCourses/tree/master/DataScience/7_RegressionModels/peerAssessment/courseProject.Rmd")
 for more information.

Parts of analysis:

- Exploratory Data Analysis
- Model Selection and Uncertainty Quantification
- Regression Diagnostics and Variable Importance


```{r, echo=FALSE, message=FALSE}
library(ggplot2)
library(caret)
library(dplyr)
library(corrplot)
library(gridExtra)

data(mtcars)
```

```{r, echo=FALSE}
features <- mtcars[,-1]
response <- mtcars$mpg
auto_data <- filter(mtcars, am == 0)
man_data <- filter(mtcars, am == 1)
```

Exploratory Analyis
---

First let's have a look at the difference in fuel consumption between cars with manual and automatic transmission.

```{r, echo=FALSE}
## Automatic Transmission
summary(auto_data$mpg)
## Manual Transmission
summary(man_data$mpg)
```

```{r, echo=FALSE}
diff <- aggregate(mpg ~ am, mtcars, median)
d <- abs(diff$mpg[1]-diff$mpg[2])
t_stat <- t.test(man_data$mpg,auto_data$mpg)

t_stat
```

The mean difference of **7.3** between cars with automatic and manual transmission is significant [Fig.: 1] with about **5.5 mpg [p = 0.0014]** and a **95% confidence interval of [3.2097, 11.28]**, with a lower fuel consumption for cars with manual transmission. It is unclear however, which features are responsible for this difference. Before we answer this question, using a number of different linear models, we will compute the correlation of each of the features provided in the mtcars dataset with the response.

```{r, echo=FALSE}
cor(response, features)
```

The number of cylinders, displacement, horsepower and the weight of the cars show a highly negative correlation with the output/response variable, which indicate a strong influence on the fuel consumption. The pairwise correlation between all variables [Fig.: 3] have been computed and show **strong colinearities between the variables disp, wt and cyl**, indicating equal importances on the fuel consumption. 

Model Selection and Uncertainty Quantification
---

To answer the question which and how many variables have an influence on the fuel consumption, a number of linear models have been fit to the data. To ensure robustness of the results, we performed a bootstrapped cross-validation scheme. Besides a naive model that included all variables, forward, backward and stepwise selection methods have been applied and their performance [Fig.: 4] and similarity [Fig.: 5] have been compared.

```{r, echo=FALSE}
set.seed(42)
ctrl <- trainControl(method = "repeatedcv",
                     number= 5,
                     repeats=5)

fit_normal <- train(x = features, y = response,
                method = "lm",
                trControl = ctrl,
                preProc = c("center", "scale"))

fit_backward <- train(x = features, y = response,
                method = "leapBackward",
                trControl = ctrl,
                preProc = c("center", "scale"))

fit_forward <- train(x = features, y = response,
                      method = "leapForward",
                      trControl = ctrl,
                      preProc = c("center", "scale"))

fit_stepwise <- train(x = features, y = response,
              method = "leapSeq",
              trControl = ctrl,
              preProc = c("center", "scale"))

results_normal <- data.frame(Observed=response,
                      Predicted=predict(fit_normal),
                      Residuals=resid(fit_normal))

results_backward <- data.frame(Observed=response,
                      Predicted=predict(fit_backward),
                      Residuals=resid(fit_backward))

results_forward <- data.frame(Observed=response,
                              Predicted=predict(fit_forward),
                              Residuals=resid(fit_forward))

results_stepwise <- data.frame(Observed=response,
                              Predicted=predict(fit_stepwise),
                              Residuals=resid(fit_stepwise))
```

```{r, echo=FALSE}
resamps <- resamples(list(normal = fit_backward,
                          backward = fit_backward,
                          forward = fit_forward,
                          stepwise = fit_stepwise))
summary(resamps)
```

The best performance could be achieved with the stepwise selection approach, which **explained 80.89%** of the variance and resulted in a **median RMSE of 3.021**. Even though explained variance in the forward selection approach was higher, this was accompanied by a higher RMSE. Note that, due to the prediction on the training data, during cross-validation, these values might be optimistic and more samples would be needed for more realistic performance measures. 

Since models are fit on the same versions of the training data, it makes sense to make inferences on the differences between models. In this way we reduce the within-resample correlation that may exist. We can compute the differences, then use a simple t-test to evaluate the null hypothesis that there is no difference between models.

```{r, echo=FALSE}
difValues <- diff(resamps)
summary(difValues)
```

The models show high similarities [Fig.: 5], with difference estimates between **-0.1797 (normal - forward)** and **0.2756 (forward -stepwise)**.

Diagnostics and Variable Importance
---

Given the model performance measures and similarity estimates, the **model that best explaines the mpg variable is the stepwise feature selection model** [Fig.: 3]. It includes all variables other than gear and the **three most important variables to explain fuel consumption, are displacement, weight and horsepower**[Fig.: 6].

Appendix
---

## Fig 1 - Automatic and Manual Transmission
```{r fig1, echo=FALSE, fig.width=8, fig.height=3}
ggplot(mtcars, aes(x=as.factor(mtcars$am), 
                   y=mtcars$mpg)) + 
  geom_boxplot() +
  geom_jitter(size=2, alpha=0.6) +
  scale_x_discrete(breaks=c(0 , 1), 
                   labels=c("automatic", "manual")) +
  labs(title = "Influence of Transmission Type\non Fuel Consumption") +
  labs(x="", y = "Miles per Gallon") + 
  theme_bw()
```

## Fig 2 - Pairwise Correlation
```{r fig2, echo=FALSE, fig.width=3, fig.height=3}
featCorr <- cor(features)

corrplot::corrplot(featCorr, 
                   order = "hclust", 
                   tl.cex = .8)
```

## Fig 3 - Diagnostic Plots

```{r fig3, echo=FALSE, fig.width=8, fig.height=3}
# ## Observed versus predicted
# p1 <- ggplot(data=results_normal, aes(x=Predicted, y=Observed)) +
#     geom_point() +
#     geom_abline(intercept=0, slope=1, linetype=2) +
#     labs(title = "Normal") +
#     theme_bw()
# 
# ## Residual Plot
# p2 <- ggplot(data=results_normal, aes(x=Predicted, y=Residuals)) +
#     geom_hline(linetype=2)+
#     geom_point() +
#     labs(title = "Normal") +
#     theme_bw()
# 
# ## Observed versus predicted
# p3 <- ggplot(data=results_backward, aes(x=Predicted, y=Observed)) +
#     geom_point() +
#     geom_abline(intercept=0, slope=1, linetype=2) +
#     labs(title = "Backward Selection") +
#     theme_bw()
# 
# ## Residual Plot
# p4 <- ggplot(data=results_backward, aes(x=Predicted, y=Residuals)) +
#     geom_hline(linetype=2)+
#     geom_point() +
#     labs(title = "Backward Selection") +
#     theme_bw()
# 
# p5 <- ggplot(data=results_forward, aes(x=Predicted, y=Observed)) +
#     geom_point() +
#     geom_abline(intercept=0, slope=1, linetype=2) +
#     labs(title = "Forward Selection") +
#     theme_bw()
# 
# ## Residual Plot
# p6 <- ggplot(data=results_forward, aes(x=Predicted, y=Residuals)) +
#     geom_hline(linetype=2)+
#     geom_point() +
#     labs(title = "Forward Selection") +
#     theme_bw()
# 
p7 <- ggplot(data=results_stepwise, aes(x=Predicted, y=Observed)) +
    geom_point() +
    geom_abline(intercept=0, slope=1, linetype=2) +
    labs(title = "Stepwise Selection") +
    theme_bw()

## Residual Plot
p8 <- ggplot(data=results_stepwise, aes(x=Predicted, y=Residuals)) +
    geom_hline(linetype=2)+
    geom_point() +
    labs(title = "Stepwise Selection") +
    theme_bw()
# 
# grid.arrange(p1, p2, 
#              p3, p4,
#              p5, p6,
#              p7, p8,
#              ncol=2)

grid.arrange(p7, p8, ncol=2) 
```

## Fig 4 - Model Performance

```{r fig4, echo=FALSE, message=FALSE, fig.width=8, fig.height=3}
bwplot(resamps, layout = c(2, 1))
```

## Fig 5 - Model Similarity

```{r fig5, echo=FALSE, message=FALSE, fig.width=8, fig.height=3}
bwplot(difValues, layout = c(3, 1))
```

## Fig 6 - Variable Importance
```{r fig6, echo=FALSE, message=FALSE, fig.width=8, fig.height=3}
plot(varImp(fit_stepwise, scale=TRUE))
```
