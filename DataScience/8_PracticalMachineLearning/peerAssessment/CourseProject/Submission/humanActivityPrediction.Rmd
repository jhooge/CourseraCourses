---
title: "Practical Machine Learning - Coursera Peer Assessment"
author: "Jens Hooge"
date: "22.6.2014"
output: html_document
---

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

# Data 

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

# Data Processing

## Loading Packages

```{r Load packages}
library(caret)     ## for the actual Machine Learning
library(e1071)     ## for the skewness calculation
library(stats)     ## predict function
library(reshape2)  ## for melting data frames
library(gridExtra) ## grid Plotting
```

The package doMC will enable multi-threading. Remove the following lines if you CPU does not support multi-threading.

```{r Load parallelization package}
library(doMC)
NTHREADS <- 30 ## set this value depending on the number of threads you want to reserve
registerDoMC(cores=NTHREADS)
```

## Functions for Pre-Processing

For the data preprocessing I will provide a few convenience function, which will take care of zero variance features as well as a function to convert factor variables to numeric variables. They also include two plotting functions. While skeewnessPlot will plot a histogram of skewness values, varDistPlot will plot the value distribution for each feature. The input for all of those functions is an nxm feature matrix data frame, where n is the number of samples and m is the number of features.

```{r Covenience functions for preprocessing}
isZeroVariance <- function(X) {
  return(apply(X, 2, function(x) length(unique(x)) == 1))
}

defactorize <-function(X) {
  isFactor <- sapply(X, class) == "factor"
  X[, isFactor] <- as.numeric(as.character(X[, isFactor]))
  return(X)
}

skewnessPlot <- function(X) {
  skewValues <- as.data.frame(sapply(X, skewness, na.rm=TRUE, type=1))
  skewValues$variable <- rownames(skewValues)
  rownames(skewValues) <- NULL
  colnames(skewValues) <- c("skewnessValue", "variable")
  skewValues$variable <- as.factor(skewValues$variable)
  
  fig <- ggplot(skewValues, aes(x=variable, y=skewnessValue)) +
    geom_bar(stat="identity", position="dodge") +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  return(fig)
}
  
varDistPlot <- function(X) {
  X_molten <- melt(X)
  fig <- ggplot(X_molten, aes(x = value)) + 
    facet_wrap(~ variable, scales = "free_x") + 
    geom_histogram() +
    theme_bw()
  return(suppressMessages(print(fig)))
}
```

## Loading Data

```{r}
X_test <- read.csv("../data//pml-testing.csv", stringsAsFactors=TRUE)
X_train <- read.csv("../data//pml-training.csv", stringsAsFactors=TRUE)
```

## Feature Extraction

The feature matrix includes 160 features, but the ones we're interested in are just the accelerators. The matrix also includes the label vector, which will be stored on it's own.

````{r Split Label}
Y_train <- X_train$classe
```

Also remove the first few columns, as they seem to be measurement device specific. In case the features, I'm going to extract, do not include enough information (variance) for the actual classification task, I will keep cleaned up training and test matrix.


```{r Clean Up}
X_train <- subset(X_train, select=-c(X, raw_timestamp_part_1,
                                     raw_timestamp_part_2, cvtd_timestamp,
                                     new_window, num_window))
X_test <- subset(X_test, select=-c(X, raw_timestamp_part_1,
                                   raw_timestamp_part_2, cvtd_timestamp,
                                   new_window, num_window))
```

The feature matrices that only include the accelerometer data, will be stored in X_train_selected and X_test_selected.

```{r Feature Extraction}
accelerometers <- colnames(X_train)[grep("accel", colnames(X_train))]
X_train_selected <- subset(X_train, select=accelerometers)
X_test_selected <- subset(X_test, select=accelerometers)
```

## Data Cleaning

First I will cast all factor variables to numeric columns. Because the label vector should stay a factor variable, it will be added after the casting. By defactorizing the matrices all missing values, that are not indicated by NA, will be replaced with NA.

```{r Defactorize}
X_train <- defactorize(X_train[,2:ncol(X_train)])
X_train_selected <- defactorize(X_train_selected[,2:ncol(X_train_selected)])
X_test_selected <- defactorize(X_test_selected[,2:ncol(X_test_selected)])
```

In a second step I will remove all features with zero variance or in other words, only include one unique value. These features almost no information to a predictive model, such the we can remove them. Filtering those features using the isZeroVariance function above, will also take care of feature, which dont' include any values at all.

```{r Remove Zero Variance Features}
X_train <- X_train[, !isZeroVariance(X_train)]
X_train_selected <- X_train_selected[, !isZeroVariance(X_train_selected)]
X_test_selected <- X_test_selected[, !isZeroVariance(X_test_selected)]
```

Now let's have a first peek at our training data. The Skewness plots will give us an idea how symmetrical the distribution of features are.

- A symmetrical distribution has a skewness of zero.

- An asymmetrical distribution with a long tail to the right (higher values) has a positive skew.

- An asymmetrical distribution with a long tail to the left (lower values) has a negative skew.

```{r fig1, fig.cap="Skewness Plot"}
skewnessPlot(X_train_selected)
```

Apart from the "var_" features everything looks relatively symmetric. So let's have a look at the distribution of the feature values.

```{r fig2, fig.cap="Feature Distribution Plot"}
varDistPlot(X_train_selected)
```

Obviously there are not many values for the problematic "var_" features, which explains the high skewness value. These features just seem to indicate the variance of the accelerator data. The distribution plot shows us more however. Aparently not all features are centered around the same value, nor are they on the same scale. The skewness problem can be solved by Box-Cox transformation. I will also center and scale the data, as this might be advantageous for the following training steps. Because some models have problems with missing values, I will aimpute those using k-neares neighbor imputation. The transformed feature matrix will be stored in X_trans.

```{r Preprocessing}
X_train_selected <- subset(X_train_selected, 
                           select=-c(var_total_accel_belt,
                                     var_accel_arm,
                                     var_accel_dumbbell, 
                                     var_accel_forearm))

preProc <- preProcess(X_train_selected, c("knnImpute", "BoxCox", 
                                          "center", "scale"))
X_trans <- predict(preProc, X_train_selected)
```

Now let's look at thos plots again:

```{r fig3, fig.cap="Skewness Plot"}
skewnessPlot(X_trans)
```

```{r fig4, fig.cap="Feature Distribution Plot"}
varDistPlot(X_trans)
```

The Box-Cox Transformation did not resolve all of the assymmetries, but that should be fine. The values are now all on the same scale and centered around zero.

# Training

I will train a k nearest neighbor and a naive Bayes classifier to predict the categories of our label vector. There is also code for a Support Vector Machine with a linear kernel, but it is still running as I write this report, such that I can not provide the RData object for the trained model, yet. Feel free to run it yourself.

```{r Lod the Models}
load("knnModel.RData")
load("naiveBayesModel.RData")
```

## Naive Bayes
I will train all the models with a 10 fold cross validation which will be bootstrapped ten times. This is a good runtime/accuracy tradeoff. For naive Bayes, per default two models will be trained one with a gaussian and with no kernel.

```{r}
# ctrl <- trainControl(
#   method = "repeatedcv",  # cross-validation method
#   number = 10,            # number of folds
#   repeats = 10,           # number of complete sets of folds
#   allowParallel = TRUE)
# 
# naiveBayesModel <- train(X_trans, Y_train, 
#                          method = "nb", 
#                          trControl = ctrl)
naiveBayesModel
# save(file="naiveBayesModel.RData", data=naiveBayesModel)
```

```{r fig5}
plot(naiveBayesModel, type="h")
```

## k-Nearest Neighbor

```{r kNN}
# grid <- expand.grid(.k = seq(1, 10, length=10))
# 
# knnModel <- train(X_trans, Y_train, 
#                   method = "knn", 
#                   trControl = ctrl,
#                   tuneGrid = grid)
knnModel
# save(file="knnModel.RData", data=knnModel)
```

```{r}
plot(knnModel)
```

## Linear-Kernel SVM

For the Linear-Kernel we can define the tuning parameter C. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the trainingpoints classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable. (http://stats.stackexchange.com/a/31067/46152)
```{r SVM}
# grid <- expand.grid(.C = seq(10^(-5), 10^5, length=10))
# 
# svmLinearModel <- train(X_trans, Y_train, 
#                         method = "svmLinear", 
#                         trControl = ctrl,
#                         tuneGrid = grid)
# svmLinearModel
# save(file="svmLinearModel.RData", data=svmLinearModel)
# plot(svmLinearModel)
```

We can see that the final naive Bayes model converges at a accuracy of about 0.6, while the knn model can achieve near perfect accuracy with k=1. **The out-of-sample error can be estimated by 1-accuracy of the cross-validation**. For the actual model selection step I will use the resamples function from caret, which makes it easy to compare different perfomance measures for each of the trained models. 

# Model Selection
```{r Model Selection}
resamps <- resamples(list(NaiveBayes = naiveBayesModel,
                          knn = knnModel))
summary(resamps)
```

```{r fig6}
trellis.par.set(caretTheme())
splom(resamps, alpha=0.3)
```

In the scatter plot matrix it should be obvious that the knn models are consistently (by cross-validation) superior to the naive Bayes model.

# Prediction

To predict the label vector with the final models, as well from knn and naive Bayes, I will use the predict function from caret and store the predicted vectors in a data frame testing. Because the prediction matrix also includes missing values I will omit those.

```{r, warning=FALSE}
p1 <- caret::predict(knnModel, na.omit(X_test_selected))
p2 <- caret::predict(naiveBayesModel, na.omit(X_test_selected))
testing <- data.frame(knn=p1, naiveBayes=p2)
testing$knn <- as.character(testing$knn)
testing$naiveBayes <- as.character(testing$naiveBayes)
```

And here we go, the predicted label vectors of both models:

```{r Prediction Values}
testing$knn
testing$naiveBayes
```
